{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0ef70fa",
   "metadata": {},
   "source": [
    "# Promoter Prediction with MergeDNA\n",
    "\n",
    "This notebook demonstrates how to use the **MergeDNA** model for promoter prediction - a binary classification task to predict whether a DNA sequence contains a promoter region.\n",
    "\n",
    "**Promoters** are DNA sequences upstream of genes that initiate transcription. Identifying them computationally is crucial for understanding gene regulation.\n",
    "\n",
    "## What we'll cover:\n",
    "1. Loading data utilities from `mergedna.data`\n",
    "2. Using the MergeDNA classifier from `mergedna.model`\n",
    "3. Training on synthetic promoter data\n",
    "4. Evaluation and visualization\n",
    "5. Inference using `mergedna.scripts.inference`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2818ccac",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6daeb0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add mergedna to path\n",
    "sys.path.insert(0, '../mergedna')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b58795e",
   "metadata": {},
   "source": [
    "## 2. Load Data Utilities from MergeDNA\n",
    "\n",
    "We use the `DNATokenizer` and `PromoterDataset` from `mergedna.data.dataloader`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f72da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import from mergedna\n",
    "from data.dataloader import (\n",
    "    DNATokenizer, \n",
    "    PromoterDataset,\n",
    "    generate_promoter_sequence,\n",
    "    generate_non_promoter_sequence,\n",
    "    generate_random_dna\n",
    ")\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = DNATokenizer()\n",
    "print(f\"Vocabulary size: {tokenizer.vocab_size}\")\n",
    "print(f\"Vocabulary: {tokenizer.vocab}\")\n",
    "\n",
    "# Test encoding/decoding\n",
    "test_seq = \"ATCGATCG\"\n",
    "encoded = tokenizer.encode(test_seq)\n",
    "decoded = tokenizer.decode(encoded)\n",
    "print(f\"\\nTest: '{test_seq}' -> {encoded.tolist()} -> '{decoded}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65f5aee",
   "metadata": {},
   "source": [
    "## 3. Create Datasets\n",
    "\n",
    "The `PromoterDataset` generates synthetic promoter and non-promoter sequences with:\n",
    "- **TATA box** motifs (`TATAAA` variants)\n",
    "- **GC-rich regions** (CpG island-like)\n",
    "- **Initiator elements**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d8fe39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show example sequences\n",
    "print(\"Example Promoter Sequence:\")\n",
    "promoter_ex = generate_promoter_sequence(256)\n",
    "print(f\"  ...{promoter_ex[40:70]}... (TATA region)\")\n",
    "print(f\"  ...{promoter_ex[75:120]}... (GC-rich region)\")\n",
    "\n",
    "print(\"\\nExample Non-Promoter Sequence:\")\n",
    "non_promoter_ex = generate_non_promoter_sequence(256)\n",
    "print(f\"  ...{non_promoter_ex[40:70]}...\")\n",
    "print(f\"  ...{non_promoter_ex[75:120]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b3fb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "SEQ_LENGTH = 256  # Must be divisible by 2^local_layers (2^2 = 4)\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dataset = PromoterDataset(num_samples=2000, seq_length=SEQ_LENGTH, tokenizer=tokenizer)\n",
    "val_dataset = PromoterDataset(num_samples=400, seq_length=SEQ_LENGTH, tokenizer=tokenizer)\n",
    "test_dataset = PromoterDataset(num_samples=400, seq_length=SEQ_LENGTH, tokenizer=tokenizer)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")\n",
    "\n",
    "# Check a batch\n",
    "batch = next(iter(train_loader))\n",
    "print(f\"\\nBatch input_ids shape: {batch['input_ids'].shape}\")\n",
    "print(f\"Batch labels shape: {batch['label'].shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430f799d",
   "metadata": {},
   "source": [
    "## 4. Load MergeDNA Classifier\n",
    "\n",
    "We use `MergeDNAClassifier` from `mergedna.model.backbone`. This model:\n",
    "- Uses **dynamic token merging** to compress sequences\n",
    "- Applies **hierarchical encoding** (local â†’ latent)\n",
    "- Adds a **classification head** on top\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74ea228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import model from mergedna\n",
    "from model.backbone import MergeDNAClassifier\n",
    "\n",
    "# Initialize model\n",
    "model = MergeDNAClassifier(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    dim=64,\n",
    "    local_layers=2,\n",
    "    latent_layers=2,\n",
    "    heads=4,\n",
    "    mlp_dim=256,\n",
    "    num_classes=2,\n",
    "    dropout=0.1,\n",
    "    pooling='mean'\n",
    ").to(device)\n",
    "\n",
    "# Count parameters\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "num_trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters: {num_params:,}\")\n",
    "print(f\"Trainable parameters: {num_trainable:,}\")\n",
    "\n",
    "# Test forward pass\n",
    "test_input = batch['input_ids'].to(device)\n",
    "test_output = model(test_input)\n",
    "print(f\"\\nInput shape: {test_input.shape}\")\n",
    "print(f\"Output shape: {test_output.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966c3ed4",
   "metadata": {},
   "source": [
    "## 5. Training Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8ae5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, optimizer, criterion, device):\n",
    "    \"\"\"Train for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch in loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        logits = model(input_ids)\n",
    "        loss = criterion(logits, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        preds = logits.argmax(dim=-1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "    \n",
    "    return total_loss / len(loader), correct / total\n",
    "\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    \"\"\"Evaluate the model.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            logits = model(input_ids)\n",
    "            loss = criterion(logits, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            preds = logits.argmax(dim=-1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    return total_loss / len(loader), correct / total, all_preds, all_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e146d0f1",
   "metadata": {},
   "source": [
    "## 6. Training Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cc2d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "NUM_EPOCHS = 15\n",
    "LEARNING_RATE = 1e-3\n",
    "WEIGHT_DECAY = 1e-4\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS)\n",
    "\n",
    "# Training history\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "    'val_loss': [],\n",
    "    'val_acc': []\n",
    "}\n",
    "\n",
    "best_val_acc = 0.0\n",
    "CHECKPOINT_PATH = 'best_promoter_model.pt'\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "    val_loss, val_acc, _, _ = evaluate(model, val_loader, criterion, device)\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    \n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        # Save best model\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_acc': val_acc,\n",
    "        }, CHECKPOINT_PATH)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1:2d}/{NUM_EPOCHS} | \"\n",
    "          f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | \"\n",
    "          f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(f\"Best validation accuracy: {best_val_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457d5dda",
   "metadata": {},
   "source": [
    "## 7. Training Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8b7403",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Loss plot\n",
    "axes[0].plot(history['train_loss'], label='Train Loss', color='#2ecc71', linewidth=2)\n",
    "axes[0].plot(history['val_loss'], label='Val Loss', color='#e74c3c', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training & Validation Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy plot\n",
    "axes[1].plot(history['train_acc'], label='Train Acc', color='#2ecc71', linewidth=2)\n",
    "axes[1].plot(history['val_acc'], label='Val Acc', color='#e74c3c', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Training & Validation Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2330132",
   "metadata": {},
   "source": [
    "## 8. Test Set Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd94edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model and evaluate on test set\n",
    "checkpoint = torch.load(CHECKPOINT_PATH)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "print(f\"Loaded model from epoch {checkpoint['epoch']+1} with val_acc: {checkpoint['val_acc']:.4f}\")\n",
    "\n",
    "test_loss, test_acc, test_preds, test_labels = evaluate(model, test_loader, criterion, device)\n",
    "\n",
    "print(f\"\\nTest Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a9d478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "cm = confusion_matrix(test_labels, test_preds)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,\n",
    "            xticklabels=['Non-Promoter', 'Promoter'],\n",
    "            yticklabels=['Non-Promoter', 'Promoter'])\n",
    "ax.set_xlabel('Predicted')\n",
    "ax.set_ylabel('Actual')\n",
    "ax.set_title('Confusion Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(test_labels, test_preds, \n",
    "                          target_names=['Non-Promoter', 'Promoter']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c5f541",
   "metadata": {},
   "source": [
    "## 9. Inference with MergeDNA Utilities\n",
    "\n",
    "Use the inference utilities from `mergedna.scripts.inference` for easy predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a825b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import inference utilities\n",
    "from scripts.inference import predict_promoter, batch_predict, get_embeddings\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"INFERENCE EXAMPLES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Test on new sequences\n",
    "new_promoter = generate_promoter_sequence(256)\n",
    "result = predict_promoter(model, new_promoter, tokenizer, device)\n",
    "print(f\"\\n[Generated Promoter Sequence]\")\n",
    "print(f\"  Preview: {result['sequence_preview']}\")\n",
    "print(f\"  Prediction: {result['prediction']}\")\n",
    "print(f\"  Confidence: {result['confidence']:.4f}\")\n",
    "\n",
    "new_non_promoter = generate_non_promoter_sequence(256)\n",
    "result = predict_promoter(model, new_non_promoter, tokenizer, device)\n",
    "print(f\"\\n[Generated Non-Promoter Sequence]\")\n",
    "print(f\"  Preview: {result['sequence_preview']}\")\n",
    "print(f\"  Prediction: {result['prediction']}\")\n",
    "print(f\"  Confidence: {result['confidence']:.4f}\")\n",
    "\n",
    "# Test with explicit TATA box\n",
    "tata_seq = \"GCGCGCGCGCATATATATATATAAAAAAAATCAGTTGCGCGCGCGCGCGC\" + generate_random_dna(206)\n",
    "result = predict_promoter(model, tata_seq, tokenizer, device)\n",
    "print(f\"\\n[Sequence with TATA-like motif]\")\n",
    "print(f\"  Preview: {result['sequence_preview']}\")\n",
    "print(f\"  Prediction: {result['prediction']}\")\n",
    "print(f\"  Confidence: {result['confidence']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015b2753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch prediction example\n",
    "test_sequences = [\n",
    "    generate_promoter_sequence(256),\n",
    "    generate_promoter_sequence(256),\n",
    "    generate_non_promoter_sequence(256),\n",
    "    generate_non_promoter_sequence(256),\n",
    "]\n",
    "\n",
    "results = batch_predict(model, test_sequences, tokenizer, device)\n",
    "\n",
    "print(\"Batch Predictions:\")\n",
    "for i, res in enumerate(results):\n",
    "    print(f\"  Seq {i+1}: {res['prediction']:12s} (confidence: {res['confidence']:.4f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7397b313",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
