{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55f8a362",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81b681d",
   "metadata": {},
   "source": [
    "## setup and dataloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fbff5cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sanjukta/probable-adventure/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"..\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "\n",
    "from mergedna.dataloader import dataloader, merge_sequences\n",
    "from mergedna.backbone import MergeDNAModel\n",
    "from mergedna.merging import MergeDNALayer, MergeDNAUnmerge\n",
    "\n",
    "# Check device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "269b0dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "DATA_DIR = \"../data/human_nontata_promoters/\"\n",
    "CHECKPOINT_PATH = \"../checkpoints/mergedna_pretrain.pt\"\n",
    "\n",
    "# Create checkpoint directory\n",
    "import os\n",
    "os.makedirs(\"../checkpoints\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e48390",
   "metadata": {},
   "outputs": [],
   "source": [
    "#variables\n",
    "batch = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa43771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading sequences...\n",
      "test\n",
      "positive\n",
      "negative\n",
      "train\n",
      "positive\n",
      "negative\n",
      "Total sequences loaded: 36131\n",
      "Sample sequence length: 251\n",
      "Sample sequence: CAGGAATCGAACACTAGGAATCCTACTCGATAGGTGGCACGGATTGCGGA...\n"
     ]
    }
   ],
   "source": [
    "pretrain_sequences = dataloader(directory_path)\n",
    "sequences = merge_sequences(pretrain_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d4c485",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc34c763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 36131\n",
      "Sample tensor shape: torch.Size([251])\n",
      "Sample tensor values: tensor([1, 0, 2, 2, 0, 0, 3, 1, 2, 0, 0, 1, 0, 1, 3, 0, 2, 2, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "# DNA vocabulary mapping\n",
    "DNA_VOCAB = {'A': 0, 'C': 1, 'G': 2, 'T': 3, 'N': 0}  # N maps to A as fallback\n",
    "\n",
    "class DNADataset(Dataset):\n",
    "    \"\"\"Dataset for DNA sequences.\"\"\"\n",
    "    def __init__(self, sequences, max_len=251):\n",
    "        self.sequences = sequences\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.sequences[idx].strip().upper()\n",
    "        # Convert to indices\n",
    "        indices = [DNA_VOCAB.get(base, 0) for base in seq[:self.max_len]]\n",
    "        # Pad if necessary\n",
    "        if len(indices) < self.max_len:\n",
    "            indices += [0] * (self.max_len - len(indices))\n",
    "        return torch.tensor(indices, dtype=torch.long)\n",
    "\n",
    "# Test the dataset\n",
    "dataset = DNADataset(all_sequences, max_len=MAX_SEQ_LEN)\n",
    "print(f\"Dataset size: {len(dataset)}\")\n",
    "print(f\"Sample tensor shape: {dataset[0].shape}\")\n",
    "print(f\"Sample tensor values: {dataset[0][:20]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3cf9ba8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([1, 251])\n",
      "Latent shape: torch.Size([1, 126, 64])\n",
      "Source map shape: torch.Size([1, 251])\n",
      "Reconstructed shape: torch.Size([1, 251, 64])\n"
     ]
    }
   ],
   "source": [
    "class LocalEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Local Encoder: Embeds DNA bases and performs token merging via pooling.\n",
    "    Maps N bases -> L tokens (L < N) with source matrix tracking.\n",
    "    Uses simple average pooling for robust merging.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, vocab_size=4, merge_ratio=0.5):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.merge_ratio = merge_ratio\n",
    "        self.embed = nn.Embedding(vocab_size, dim)\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, 512, dim) * 0.02)\n",
    "        \n",
    "        # Local attention for context before merging\n",
    "        self.local_attn = nn.TransformerEncoderLayer(\n",
    "            d_model=dim, nhead=4, dim_feedforward=dim*4, \n",
    "            dropout=0.1, batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Learnable merge weights\n",
    "        self.merge_proj = nn.Linear(dim, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: [B, N] indices or [B, N, 4] one-hot\n",
    "        Returns: z_l [B, L, D], source_map [B, N]\n",
    "        \"\"\"\n",
    "        if x.dim() == 3:  # one-hot\n",
    "            x_idx = x.argmax(dim=-1)\n",
    "        else:\n",
    "            x_idx = x\n",
    "        \n",
    "        B, N = x_idx.shape\n",
    "        \n",
    "        # Embed\n",
    "        z = self.embed(x_idx)  # [B, N, D]\n",
    "        z = z + self.pos_embed[:, :N, :]\n",
    "        \n",
    "        # Local attention\n",
    "        z = self.local_attn(z)\n",
    "        \n",
    "        # Simple pooling-based merging (pairs of adjacent tokens)\n",
    "        # This is more robust than bipartite matching\n",
    "        target_len = max(int(N * self.merge_ratio), 1)\n",
    "        \n",
    "        # Merge by averaging pairs of tokens\n",
    "        # Pad to even length if needed\n",
    "        if N % 2 == 1:\n",
    "            z = F.pad(z, (0, 0, 0, 1))  # Pad sequence dim\n",
    "            N_padded = N + 1\n",
    "        else:\n",
    "            N_padded = N\n",
    "        \n",
    "        # Reshape and average pairs: [B, N, D] -> [B, N/2, 2, D] -> [B, N/2, D]\n",
    "        z_pairs = z.view(B, N_padded // 2, 2, self.dim)\n",
    "        z_merged = z_pairs.mean(dim=2)  # [B, N/2, D]\n",
    "        \n",
    "        # Build source map: each original token points to its merged token\n",
    "        # Token 0,1 -> 0, Token 2,3 -> 1, etc.\n",
    "        source_map = torch.arange(N, device=z.device) // 2\n",
    "        source_map = source_map.unsqueeze(0).expand(B, -1).clone()  # [B, N]\n",
    "        \n",
    "        return z_merged, source_map\n",
    "\n",
    "\n",
    "class LocalDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Local Decoder: Unmerges tokens back to original length and refines.\n",
    "    Maps L tokens -> N bases using source matrix.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        \n",
    "        # Refinement layers after unmerging\n",
    "        self.refine = nn.TransformerEncoderLayer(\n",
    "            d_model=dim, nhead=4, dim_feedforward=dim*4,\n",
    "            dropout=0.1, batch_first=True\n",
    "        )\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "    \n",
    "    def forward(self, z_l, source_map):\n",
    "        \"\"\"\n",
    "        z_l: [B, L, D] merged tokens\n",
    "        source_map: [B, N] ownership map (indices into L)\n",
    "        Returns: x_hat [B, N, D]\n",
    "        \"\"\"\n",
    "        B, L, D = z_l.shape\n",
    "        N = source_map.shape[1]\n",
    "        \n",
    "        # Unmerge: broadcast L tokens back to N positions using source_map\n",
    "        indices = source_map.unsqueeze(-1).expand(-1, -1, D)  # [B, N, D]\n",
    "        x_unmerged = torch.gather(z_l, 1, indices)  # [B, N, D]\n",
    "        \n",
    "        # Refine\n",
    "        x_refined = self.refine(x_unmerged)\n",
    "        \n",
    "        return self.proj(x_refined)\n",
    "\n",
    "# Test the components\n",
    "test_input = dataset[0].unsqueeze(0).to(device)\n",
    "local_enc = LocalEncoder(DIM).to(device)\n",
    "local_dec = LocalDecoder(DIM).to(device)\n",
    "\n",
    "z_l, s_map = local_enc(test_input)\n",
    "x_hat = local_dec(z_l, s_map)\n",
    "\n",
    "print(f\"Input shape: {test_input.shape}\")\n",
    "print(f\"Latent shape: {z_l.shape}\")\n",
    "print(f\"Source map shape: {s_map.shape}\")\n",
    "print(f\"Reconstructed shape: {x_hat.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd261a05",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3cb0cb9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 442,581\n",
      "Trainable parameters: 442,581\n"
     ]
    }
   ],
   "source": [
    "# Initialize model components\n",
    "local_encoder = LocalEncoder(DIM, merge_ratio=0.5).to(device)\n",
    "local_decoder = LocalDecoder(DIM).to(device)\n",
    "\n",
    "# Create the full MergeDNA model\n",
    "model = MergeDNAModel(\n",
    "    local_encoder=local_encoder,\n",
    "    local_decoder=local_decoder,\n",
    "    dim=DIM,\n",
    "    latent_enc_depth=LATENT_ENC_DEPTH,\n",
    "    latent_dec_depth=LATENT_DEC_DEPTH,\n",
    "    vocab_size=4\n",
    ").to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1d52c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches: 2259\n",
      "Batch size: 16\n"
     ]
    }
   ],
   "source": [
    "# Create DataLoader\n",
    "train_loader = DataLoader(\n",
    "    dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True, \n",
    "    num_workers=0,\n",
    "    pin_memory=True if device.type == 'cuda' else False\n",
    ")\n",
    "\n",
    "print(f\"Number of batches: {len(train_loader)}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc23afc",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a567af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, optimizer, device, lambda_latent=0.25):\n",
    "    \"\"\"Train for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    \n",
    "    epoch_losses = {'total': [], 'mtr': [], 'latent': [], 'amtm': []}\n",
    "    \n",
    "    pbar = tqdm(dataloader, desc=\"Training\")\n",
    "    for batch_idx, batch in enumerate(pbar):\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        try:\n",
    "            loss, logs = model.forward_train(batch, lambda_latent=lambda_latent)\n",
    "        except Exception as e:\n",
    "            print(f\"Error in batch {batch_idx}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_losses['total'].append(loss.item())\n",
    "        epoch_losses['mtr'].append(logs['loss_mtr'])\n",
    "        epoch_losses['latent'].append(logs['loss_latent'])\n",
    "        epoch_losses['amtm'].append(logs['loss_amtm'])\n",
    "        \n",
    "        pbar.set_postfix({\n",
    "            'loss': f\"{loss.item():.4f}\",\n",
    "            'mtr': f\"{logs['loss_mtr']:.3f}\",\n",
    "            'amtm': f\"{logs['loss_amtm']:.3f}\"\n",
    "        })\n",
    "    \n",
    "    return {k: np.mean(v) for k, v in epoch_losses.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ecbe382b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting pre-training...\n",
      "============================================================\n",
      "\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/2259 [00:13<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPOCHS):\n\u001b[32m     14\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m     epoch_losses = \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambda_latent\u001b[49m\u001b[43m=\u001b[49m\u001b[43mLAMBDA_LATENT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m     scheduler.step()\n\u001b[32m     19\u001b[39m     current_lr = scheduler.get_last_lr()[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36mtrain_epoch\u001b[39m\u001b[34m(model, dataloader, optimizer, device, lambda_latent)\u001b[39m\n\u001b[32m     15\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError in batch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     16\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=\u001b[32m1.0\u001b[39m)\n\u001b[32m     20\u001b[39m optimizer.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/probable-adventure/.venv/lib/python3.12/site-packages/torch/_tensor.py:625\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    617\u001b[39m         Tensor.backward,\n\u001b[32m    618\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    623\u001b[39m         inputs=inputs,\n\u001b[32m    624\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/probable-adventure/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/probable-adventure/.venv/lib/python3.12/site-packages/torch/autograd/graph.py:841\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Initialize optimizer and scheduler\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.01)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS, eta_min=1e-6)\n",
    "\n",
    "# Training history\n",
    "history = {'total': [], 'mtr': [], 'latent': [], 'amtm': [], 'lr': []}\n",
    "\n",
    "print(\"Starting pre-training...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "best_loss = float('inf')\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n",
    "    \n",
    "    epoch_losses = train_epoch(model, train_loader, optimizer, device, lambda_latent=LAMBDA_LATENT)\n",
    "    \n",
    "    scheduler.step()\n",
    "    current_lr = scheduler.get_last_lr()[0]\n",
    "    \n",
    "    for key in ['total', 'mtr', 'latent', 'amtm']:\n",
    "        history[key].append(epoch_losses[key])\n",
    "    history['lr'].append(current_lr)\n",
    "    \n",
    "    print(f\"  Total: {epoch_losses['total']:.4f} | MTR: {epoch_losses['mtr']:.4f} | Latent: {epoch_losses['latent']:.4f} | AMTM: {epoch_losses['amtm']:.4f}\")\n",
    "    \n",
    "    if epoch_losses['total'] < best_loss:\n",
    "        best_loss = epoch_losses['total']\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': best_loss,\n",
    "            'history': history\n",
    "        }, CHECKPOINT_PATH)\n",
    "        print(f\"  ✓ Saved best model (loss: {best_loss:.4f})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Pre-training complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005a8468",
   "metadata": {},
   "source": [
    "## Training Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b67f859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training history from checkpoint (if training was interrupted)\n",
    "if os.path.exists(CHECKPOINT_PATH):\n",
    "    checkpoint = torch.load(CHECKPOINT_PATH, map_location=device)\n",
    "    if 'history' in checkpoint:\n",
    "        history = checkpoint['history']\n",
    "        print(f\"Loaded history from checkpoint (epoch {checkpoint['epoch'] + 1})\")\n",
    "    else:\n",
    "        print(\"No history in checkpoint, using current history\")\n",
    "else:\n",
    "    print(\"No checkpoint found, using current history\")\n",
    "\n",
    "# Plot training curves\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('MergeDNA Pre-training Losses', fontsize=16, fontweight='bold', y=1.02)\n",
    "\n",
    "colors = {\n",
    "    'total': '#2563EB',   # Blue\n",
    "    'mtr': '#DC2626',     # Red\n",
    "    'latent': '#F59E0B',  # Amber\n",
    "    'amtm': '#10B981'     # Emerald\n",
    "}\n",
    "\n",
    "epochs_range = range(1, len(history['total']) + 1)\n",
    "\n",
    "# Total Loss\n",
    "ax = axes[0, 0]\n",
    "ax.plot(epochs_range, history['total'], 'o-', color=colors['total'], linewidth=2.5, markersize=8)\n",
    "ax.fill_between(epochs_range, history['total'], alpha=0.15, color=colors['total'])\n",
    "ax.set_title('Total Loss', fontsize=13, fontweight='bold', pad=10)\n",
    "ax.set_xlabel('Epoch', fontsize=11)\n",
    "ax.set_ylabel('Loss', fontsize=11)\n",
    "ax.grid(True, alpha=0.3, linestyle='--')\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "# MTR Loss\n",
    "ax = axes[0, 1]\n",
    "ax.plot(epochs_range, history['mtr'], 's-', color=colors['mtr'], linewidth=2.5, markersize=8)\n",
    "ax.fill_between(epochs_range, history['mtr'], alpha=0.15, color=colors['mtr'])\n",
    "ax.set_title('MTR Loss (Merged Token Reconstruction)', fontsize=13, fontweight='bold', pad=10)\n",
    "ax.set_xlabel('Epoch', fontsize=11)\n",
    "ax.set_ylabel('Loss', fontsize=11)\n",
    "ax.grid(True, alpha=0.3, linestyle='--')\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "# Latent MTR Loss\n",
    "ax = axes[1, 0]\n",
    "ax.plot(epochs_range, history['latent'], '^-', color=colors['latent'], linewidth=2.5, markersize=8)\n",
    "ax.fill_between(epochs_range, history['latent'], alpha=0.15, color=colors['latent'])\n",
    "ax.set_title('Latent MTR Loss (Adaptive Selection)', fontsize=13, fontweight='bold', pad=10)\n",
    "ax.set_xlabel('Epoch', fontsize=11)\n",
    "ax.set_ylabel('Loss', fontsize=11)\n",
    "ax.grid(True, alpha=0.3, linestyle='--')\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "# AMTM Loss\n",
    "ax = axes[1, 1]\n",
    "ax.plot(epochs_range, history['amtm'], 'd-', color=colors['amtm'], linewidth=2.5, markersize=8)\n",
    "ax.fill_between(epochs_range, history['amtm'], alpha=0.15, color=colors['amtm'])\n",
    "ax.set_title('AMTM Loss (Adaptive Masked Modeling)', fontsize=13, fontweight='bold', pad=10)\n",
    "ax.set_xlabel('Epoch', fontsize=11)\n",
    "ax.set_ylabel('Loss', fontsize=11)\n",
    "ax.grid(True, alpha=0.3, linestyle='--')\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../checkpoints/training_curves.png', dpi=150, bbox_inches='tight', facecolor='white')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nTraining curves saved to ../checkpoints/training_curves.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451b7535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined loss plot with all metrics\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "ax.plot(epochs_range, history['total'], 'o-', label='Total', color=colors['total'], linewidth=2.5, markersize=7)\n",
    "ax.plot(epochs_range, history['mtr'], 's-', label='MTR', color=colors['mtr'], linewidth=2, markersize=6)\n",
    "ax.plot(epochs_range, history['latent'], '^-', label='Latent MTR', color=colors['latent'], linewidth=2, markersize=6)\n",
    "ax.plot(epochs_range, history['amtm'], 'd-', label='AMTM', color=colors['amtm'], linewidth=2, markersize=6)\n",
    "\n",
    "ax.set_xlabel('Epoch', fontsize=12)\n",
    "ax.set_ylabel('Loss', fontsize=12)\n",
    "ax.set_title('MergeDNA Pre-training: All Losses', fontsize=14, fontweight='bold')\n",
    "ax.legend(loc='upper right', fontsize=11, framealpha=0.9)\n",
    "ax.grid(True, alpha=0.3, linestyle='--')\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../checkpoints/combined_losses.png', dpi=150, bbox_inches='tight', facecolor='white')\n",
    "plt.show()\n",
    "\n",
    "# Training summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nFinal Losses (Epoch {len(history['total'])}):\")\n",
    "print(f\"  • Total:  {history['total'][-1]:.4f}\")\n",
    "print(f\"  • MTR:    {history['mtr'][-1]:.4f}\")\n",
    "print(f\"  • Latent: {history['latent'][-1]:.4f}\")\n",
    "print(f\"  • AMTM:   {history['amtm'][-1]:.4f}\")\n",
    "\n",
    "if len(history['total']) > 1:\n",
    "    improvement = (history['total'][0] - history['total'][-1]) / history['total'][0] * 100\n",
    "    print(f\"\\nImprovement: {improvement:.1f}% reduction in total loss\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4249efc",
   "metadata": {},
   "source": [
    "## Inference: Pure Reconstruction (Autoencoding)\n",
    "\n",
    "**Mode 1: Anomaly Detection via Reconstruction Error**\n",
    "\n",
    "- **Input**: Full DNA sequence (e.g., `ATGC...`)\n",
    "- **Action**: Compress → Process → Decompress\n",
    "- **Use Case**: Anomaly detection. If you feed in a mutated/unusual sequence and the model reconstructs it as a \"normal\" sequence (high reconstruction error), you know the input was anomalous.\n",
    "\n",
    "The reconstruction error can serve as an **anomaly score** — higher error indicates the sequence deviates from patterns learned during pre-training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfe551c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model from checkpoint\n",
    "def load_model_from_checkpoint(checkpoint_path, device):\n",
    "    \"\"\"Load pre-trained model from checkpoint.\"\"\"\n",
    "    \n",
    "    # Recreate model architecture\n",
    "    local_encoder = LocalEncoder(DIM, merge_ratio=0.5).to(device)\n",
    "    local_decoder = LocalDecoder(DIM).to(device)\n",
    "    \n",
    "    model = MergeDNAModel(\n",
    "        local_encoder=local_encoder,\n",
    "        local_decoder=local_decoder,\n",
    "        dim=DIM,\n",
    "        latent_enc_depth=LATENT_ENC_DEPTH,\n",
    "        latent_dec_depth=LATENT_DEC_DEPTH,\n",
    "        vocab_size=4\n",
    "    ).to(device)\n",
    "    \n",
    "    # Load weights\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "    \n",
    "    print(f\"✓ Loaded model from epoch {checkpoint['epoch'] + 1}\")\n",
    "    print(f\"  Training loss: {checkpoint['loss']:.4f}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Load the model\n",
    "inference_model = load_model_from_checkpoint(CHECKPOINT_PATH, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31c3731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DNA mapping for decoding\n",
    "IDX_TO_BASE = {0: 'A', 1: 'C', 2: 'G', 3: 'T'}\n",
    "\n",
    "def sequence_to_tensor(sequence, max_len=251):\n",
    "    \"\"\"Convert a DNA sequence string to tensor.\"\"\"\n",
    "    seq = sequence.strip().upper()\n",
    "    indices = [DNA_VOCAB.get(base, 0) for base in seq[:max_len]]\n",
    "    if len(indices) < max_len:\n",
    "        indices += [0] * (max_len - len(indices))\n",
    "    return torch.tensor(indices, dtype=torch.long)\n",
    "\n",
    "def tensor_to_sequence(tensor):\n",
    "    \"\"\"Convert tensor back to DNA sequence string.\"\"\"\n",
    "    indices = tensor.cpu().numpy()\n",
    "    return ''.join([IDX_TO_BASE[idx] for idx in indices])\n",
    "\n",
    "@torch.no_grad()\n",
    "def reconstruct_sequence(model, sequence, device):\n",
    "    \"\"\"\n",
    "    Pure reconstruction: Compress → Process → Decompress\n",
    "    Returns the reconstructed sequence and reconstruction error.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Convert to tensor\n",
    "    x = sequence_to_tensor(sequence).unsqueeze(0).to(device)  # [1, N]\n",
    "    x_onehot = F.one_hot(x, num_classes=4).float()  # [1, N, 4]\n",
    "    \n",
    "    # Forward pass through the autoencoder\n",
    "    # 1. Local Encode (compress)\n",
    "    z_l, s_local = model.local_encoder(x_onehot)\n",
    "    \n",
    "    # 2. Latent Encode (process)\n",
    "    z_prime_l = model.latent_encoder(z_l)\n",
    "    \n",
    "    # 3. Latent Decode\n",
    "    z_hat_l = model.latent_decoder(z_prime_l)\n",
    "    \n",
    "    # 4. Local Decode (decompress)\n",
    "    x_hat = model.local_decoder(z_hat_l, s_local)\n",
    "    \n",
    "    # 5. Get logits and predictions\n",
    "    logits = model.head(x_hat)  # [1, N, 4]\n",
    "    predictions = logits.argmax(dim=-1)  # [1, N]\n",
    "    \n",
    "    # Calculate reconstruction error (cross-entropy per position)\n",
    "    loss = F.cross_entropy(logits.view(-1, 4), x.view(-1), reduction='none')\n",
    "    per_position_error = loss.view(x.shape)  # [1, N]\n",
    "    \n",
    "    # Get reconstructed sequence\n",
    "    reconstructed = tensor_to_sequence(predictions[0])\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = (predictions == x).float().mean().item()\n",
    "    \n",
    "    return {\n",
    "        'original': sequence[:251],\n",
    "        'reconstructed': reconstructed,\n",
    "        'accuracy': accuracy,\n",
    "        'mean_error': per_position_error.mean().item(),\n",
    "        'per_position_error': per_position_error[0].cpu().numpy(),\n",
    "        'compression_ratio': z_l.shape[1] / x.shape[1]\n",
    "    }\n",
    "\n",
    "print(\"✓ Reconstruction functions defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe51feb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test reconstruction on a normal sequence from the dataset\n",
    "normal_sequence = all_sequences[0]\n",
    "result = reconstruct_sequence(inference_model, normal_sequence, device)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"RECONSTRUCTION TEST: Normal Sequence\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nOriginal:      {result['original'][:60]}...\")\n",
    "print(f\"Reconstructed: {result['reconstructed'][:60]}...\")\n",
    "print(f\"\\nAccuracy:         {result['accuracy']*100:.2f}%\")\n",
    "print(f\"Mean Error:       {result['mean_error']:.4f}\")\n",
    "print(f\"Compression:      {result['compression_ratio']:.2f}x ({int(251 * result['compression_ratio'])} tokens)\")\n",
    "\n",
    "# Show mismatches\n",
    "mismatches = sum(1 for a, b in zip(result['original'], result['reconstructed']) if a != b)\n",
    "print(f\"Mismatches:       {mismatches}/{len(result['original'])} positions\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86cde04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mutated_sequence(sequence, mutation_rate=0.1):\n",
    "    \"\"\"Create a mutated version of a sequence.\"\"\"\n",
    "    bases = ['A', 'C', 'G', 'T']\n",
    "    seq_list = list(sequence.upper())\n",
    "    n_mutations = int(len(seq_list) * mutation_rate)\n",
    "    \n",
    "    mutation_positions = np.random.choice(len(seq_list), n_mutations, replace=False)\n",
    "    \n",
    "    for pos in mutation_positions:\n",
    "        original_base = seq_list[pos]\n",
    "        # Pick a different base\n",
    "        new_bases = [b for b in bases if b != original_base]\n",
    "        seq_list[pos] = np.random.choice(new_bases)\n",
    "    \n",
    "    return ''.join(seq_list), mutation_positions\n",
    "\n",
    "def create_random_sequence(length=251):\n",
    "    \"\"\"Create a completely random DNA sequence.\"\"\"\n",
    "    bases = ['A', 'C', 'G', 'T']\n",
    "    return ''.join(np.random.choice(bases, length))\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_anomaly_score(model, sequence, device):\n",
    "    \"\"\"Compute anomaly score based on reconstruction error.\"\"\"\n",
    "    result = reconstruct_sequence(model, sequence, device)\n",
    "    return result['mean_error'], result\n",
    "\n",
    "print(\"✓ Anomaly detection functions defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3cdaf5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anomaly Detection Demo: Compare normal vs mutated vs random sequences\n",
    "print(\"=\"*70)\n",
    "print(\"ANOMALY DETECTION DEMO\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# 1. Normal sequences (from training data)\n",
    "normal_scores = []\n",
    "for i in range(20):\n",
    "    score, _ = compute_anomaly_score(inference_model, all_sequences[i], device)\n",
    "    normal_scores.append(score)\n",
    "\n",
    "# 2. Mutated sequences (10% mutation rate)\n",
    "mutated_scores = []\n",
    "for i in range(20):\n",
    "    mutated_seq, _ = create_mutated_sequence(all_sequences[i], mutation_rate=0.1)\n",
    "    score, _ = compute_anomaly_score(inference_model, mutated_seq, device)\n",
    "    mutated_scores.append(score)\n",
    "\n",
    "# 3. Heavily mutated sequences (30% mutation rate)\n",
    "heavily_mutated_scores = []\n",
    "for i in range(20):\n",
    "    mutated_seq, _ = create_mutated_sequence(all_sequences[i], mutation_rate=0.3)\n",
    "    score, _ = compute_anomaly_score(inference_model, mutated_seq, device)\n",
    "    heavily_mutated_scores.append(score)\n",
    "\n",
    "# 4. Random sequences\n",
    "random_scores = []\n",
    "for i in range(20):\n",
    "    random_seq = create_random_sequence(251)\n",
    "    score, _ = compute_anomaly_score(inference_model, random_seq, device)\n",
    "    random_scores.append(score)\n",
    "\n",
    "print(f\"\\nAnomaly Scores (Reconstruction Error):\")\n",
    "print(f\"  Normal sequences:         {np.mean(normal_scores):.4f} ± {np.std(normal_scores):.4f}\")\n",
    "print(f\"  Mutated (10%):            {np.mean(mutated_scores):.4f} ± {np.std(mutated_scores):.4f}\")\n",
    "print(f\"  Heavily mutated (30%):    {np.mean(heavily_mutated_scores):.4f} ± {np.std(heavily_mutated_scores):.4f}\")\n",
    "print(f\"  Random sequences:         {np.mean(random_scores):.4f} ± {np.std(random_scores):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c634132d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize anomaly score distributions\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Box plot\n",
    "ax = axes[0]\n",
    "data = [normal_scores, mutated_scores, heavily_mutated_scores, random_scores]\n",
    "labels = ['Normal', 'Mutated\\n(10%)', 'Heavily\\nMutated (30%)', 'Random']\n",
    "colors_box = ['#10B981', '#F59E0B', '#EF4444', '#6366F1']\n",
    "\n",
    "bp = ax.boxplot(data, labels=labels, patch_artist=True)\n",
    "for patch, color in zip(bp['boxes'], colors_box):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_alpha(0.7)\n",
    "\n",
    "ax.set_ylabel('Anomaly Score (Reconstruction Error)', fontsize=11)\n",
    "ax.set_title('Anomaly Score Distribution by Sequence Type', fontsize=13, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3, axis='y', linestyle='--')\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "# Bar plot with error bars\n",
    "ax = axes[1]\n",
    "means = [np.mean(s) for s in data]\n",
    "stds = [np.std(s) for s in data]\n",
    "\n",
    "bars = ax.bar(labels, means, yerr=stds, capsize=5, color=colors_box, alpha=0.8, edgecolor='black', linewidth=1.2)\n",
    "ax.set_ylabel('Mean Anomaly Score', fontsize=11)\n",
    "ax.set_title('Mean Anomaly Scores (±1 SD)', fontsize=13, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3, axis='y', linestyle='--')\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, mean in zip(bars, means):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.05, \n",
    "            f'{mean:.3f}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../checkpoints/anomaly_detection.png', dpi=150, bbox_inches='tight', facecolor='white')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nAnomaly detection plot saved to ../checkpoints/anomaly_detection.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28d115e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize per-position reconstruction error\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 8))\n",
    "\n",
    "# Normal sequence\n",
    "normal_result = reconstruct_sequence(inference_model, all_sequences[0], device)\n",
    "\n",
    "ax = axes[0]\n",
    "ax.plot(normal_result['per_position_error'], color='#10B981', linewidth=1.5, alpha=0.8)\n",
    "ax.fill_between(range(len(normal_result['per_position_error'])), \n",
    "                normal_result['per_position_error'], alpha=0.3, color='#10B981')\n",
    "ax.axhline(y=normal_result['mean_error'], color='#10B981', linestyle='--', linewidth=2, label=f'Mean: {normal_result[\"mean_error\"]:.3f}')\n",
    "ax.set_title('Per-Position Reconstruction Error: Normal Sequence', fontsize=13, fontweight='bold')\n",
    "ax.set_xlabel('Position (bp)', fontsize=11)\n",
    "ax.set_ylabel('Error', fontsize=11)\n",
    "ax.legend(loc='upper right')\n",
    "ax.grid(True, alpha=0.3, linestyle='--')\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "# Heavily mutated sequence\n",
    "mutated_seq, mutation_pos = create_mutated_sequence(all_sequences[0], mutation_rate=0.3)\n",
    "mutated_result = reconstruct_sequence(inference_model, mutated_seq, device)\n",
    "\n",
    "ax = axes[1]\n",
    "ax.plot(mutated_result['per_position_error'], color='#EF4444', linewidth=1.5, alpha=0.8)\n",
    "ax.fill_between(range(len(mutated_result['per_position_error'])), \n",
    "                mutated_result['per_position_error'], alpha=0.3, color='#EF4444')\n",
    "ax.axhline(y=mutated_result['mean_error'], color='#EF4444', linestyle='--', linewidth=2, label=f'Mean: {mutated_result[\"mean_error\"]:.3f}')\n",
    "\n",
    "# Mark mutation positions\n",
    "for pos in mutation_pos:\n",
    "    ax.axvline(x=pos, color='blue', alpha=0.3, linewidth=0.5)\n",
    "\n",
    "ax.set_title('Per-Position Reconstruction Error: Mutated Sequence (30%)', fontsize=13, fontweight='bold')\n",
    "ax.set_xlabel('Position (bp)', fontsize=11)\n",
    "ax.set_ylabel('Error', fontsize=11)\n",
    "ax.legend(loc='upper right')\n",
    "ax.grid(True, alpha=0.3, linestyle='--')\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../checkpoints/per_position_error.png', dpi=150, bbox_inches='tight', facecolor='white')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nPer-position error plot saved to ../checkpoints/per_position_error.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d16208",
   "metadata": {},
   "source": [
    "## Interactive Anomaly Detection\n",
    "\n",
    "Use this function to test any DNA sequence for anomalies:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa85e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_anomaly(sequence, threshold=None):\n",
    "    \"\"\"\n",
    "    Detect if a sequence is anomalous based on reconstruction error.\n",
    "    \n",
    "    Args:\n",
    "        sequence: DNA sequence string (A, C, G, T)\n",
    "        threshold: Anomaly threshold (if None, uses mean + 2*std of normal sequences)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with anomaly detection results\n",
    "    \"\"\"\n",
    "    # Compute threshold from normal sequences if not provided\n",
    "    if threshold is None:\n",
    "        threshold = np.mean(normal_scores) + 2 * np.std(normal_scores)\n",
    "    \n",
    "    # Get anomaly score\n",
    "    score, result = compute_anomaly_score(inference_model, sequence, device)\n",
    "    \n",
    "    is_anomaly = score > threshold\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"ANOMALY DETECTION RESULT\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\nSequence (first 50bp): {sequence[:50]}...\")\n",
    "    print(f\"Sequence length:       {len(sequence)} bp\")\n",
    "    print(f\"\\nAnomaly Score:         {score:.4f}\")\n",
    "    print(f\"Threshold:             {threshold:.4f}\")\n",
    "    print(f\"Reconstruction Acc:    {result['accuracy']*100:.2f}%\")\n",
    "    print(f\"\\nVerdict:               {'⚠️  ANOMALOUS' if is_anomaly else '✓ NORMAL'}\")\n",
    "    \n",
    "    return {\n",
    "        'is_anomaly': is_anomaly,\n",
    "        'score': score,\n",
    "        'threshold': threshold,\n",
    "        'accuracy': result['accuracy'],\n",
    "        'result': result\n",
    "    }\n",
    "\n",
    "# Example: Test a normal sequence\n",
    "print(\"Testing a NORMAL sequence from the dataset:\")\n",
    "_ = detect_anomaly(all_sequences[100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdb04aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Test a RANDOM (anomalous) sequence\n",
    "print(\"\\nTesting a RANDOM (anomalous) sequence:\")\n",
    "random_seq = create_random_sequence(251)\n",
    "_ = detect_anomaly(random_seq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e54c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try your own sequence!\n",
    "# Replace this with any DNA sequence you want to test\n",
    "\n",
    "custom_sequence = \"ATGCATGCATGCATGCATGCATGCATGCATGCATGCATGCATGCATGCATGCATGCATGC\" * 4 + \"ATG\"\n",
    "\n",
    "print(\"\\nTesting a CUSTOM sequence (repetitive pattern):\")\n",
    "_ = detect_anomaly(custom_sequence[:251])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8daa9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8634f500",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919e6da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
