{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0d75039",
   "metadata": {},
   "source": [
    "# Promoter Prediction with MergeDNA\n",
    "\n",
    "This notebook demonstrates how to use the MergeDNA model for **promoter prediction** - a binary classification task where we predict whether a DNA sequence contains a promoter region.\n",
    "\n",
    "Promoters are DNA sequences located upstream of genes that initiate transcription. Identifying them computationally is crucial for understanding gene regulation.\n",
    "\n",
    "## What we'll cover:\n",
    "1. DNA tokenization and encoding\n",
    "2. Loading the MergeDNA model\n",
    "3. Adapting the model for classification\n",
    "4. Training on synthetic promoter data\n",
    "5. Evaluation and visualization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b390a093",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6d139c7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tqdm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mrandom\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnotebook\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Set seeds for reproducibility\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tqdm'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../mergedna')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a456e2c9",
   "metadata": {},
   "source": [
    "## 2. DNA Tokenization\n",
    "\n",
    "DNA sequences are composed of 4 nucleotides: **A** (Adenine), **T** (Thymine), **C** (Cytosine), **G** (Guanine).\n",
    "\n",
    "We'll create a simple tokenizer that maps these to integer indices, plus special tokens for padding and unknown characters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f28bbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNATokenizer:\n",
    "    \"\"\"Simple DNA sequence tokenizer.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Token vocabulary\n",
    "        self.vocab = {\n",
    "            '<PAD>': 0,\n",
    "            '<UNK>': 1,\n",
    "            '<CLS>': 2,\n",
    "            '<SEP>': 3,\n",
    "            'A': 4,\n",
    "            'T': 5,\n",
    "            'C': 6,\n",
    "            'G': 7,\n",
    "            # IUPAC ambiguity codes\n",
    "            'N': 8,   # Any nucleotide\n",
    "            'R': 9,   # A or G (purine)\n",
    "            'Y': 10,  # C or T (pyrimidine)\n",
    "            'S': 11,  # G or C\n",
    "        }\n",
    "        self.inv_vocab = {v: k for k, v in self.vocab.items()}\n",
    "        self.vocab_size = len(self.vocab)\n",
    "    \n",
    "    def encode(self, sequence: str, max_length: int = None) -> torch.Tensor:\n",
    "        \"\"\"Encode a DNA sequence to token indices.\"\"\"\n",
    "        tokens = [self.vocab.get(c.upper(), self.vocab['<UNK>']) for c in sequence]\n",
    "        \n",
    "        if max_length:\n",
    "            if len(tokens) < max_length:\n",
    "                tokens += [self.vocab['<PAD>']] * (max_length - len(tokens))\n",
    "            else:\n",
    "                tokens = tokens[:max_length]\n",
    "        \n",
    "        return torch.tensor(tokens, dtype=torch.long)\n",
    "    \n",
    "    def decode(self, tokens: torch.Tensor) -> str:\n",
    "        \"\"\"Decode token indices back to DNA sequence.\"\"\"\n",
    "        return ''.join(self.inv_vocab.get(t.item(), '?') for t in tokens)\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = DNATokenizer()\n",
    "print(f\"Vocabulary size: {tokenizer.vocab_size}\")\n",
    "print(f\"Vocabulary: {tokenizer.vocab}\")\n",
    "\n",
    "# Test encoding/decoding\n",
    "test_seq = \"ATCGATCG\"\n",
    "encoded = tokenizer.encode(test_seq)\n",
    "decoded = tokenizer.decode(encoded)\n",
    "print(f\"\\nTest: '{test_seq}' -> {encoded.tolist()} -> '{decoded}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66894b28",
   "metadata": {},
   "source": [
    "## 3. Synthetic Promoter Dataset\n",
    "\n",
    "For this example, we'll create synthetic data that mimics promoter vs non-promoter sequences.\n",
    "\n",
    "**Promoter characteristics** (simplified):\n",
    "- TATA box motif (~25-35 bp upstream): `TATAAA` or variants\n",
    "- GC-rich regions near transcription start site\n",
    "- Initiator element (Inr): `YYANTYY` pattern\n",
    "\n",
    "We'll generate sequences with these patterns for positive class and random sequences for negative class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e422da7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_dna(length: int) -> str:\n",
    "    \"\"\"Generate a random DNA sequence.\"\"\"\n",
    "    return ''.join(random.choices(['A', 'T', 'C', 'G'], k=length))\n",
    "\n",
    "def generate_promoter_sequence(length: int = 256) -> str:\n",
    "    \"\"\"\n",
    "    Generate a synthetic promoter-like sequence with characteristic motifs.\n",
    "    \"\"\"\n",
    "    seq = list(generate_random_dna(length))\n",
    "    \n",
    "    # Insert TATA box (position ~200-210 from TSS, we place it around position 50-60)\n",
    "    tata_variants = ['TATAAA', 'TATATA', 'TATAAG', 'TATAAT']\n",
    "    tata_pos = random.randint(45, 55)\n",
    "    tata_seq = random.choice(tata_variants)\n",
    "    for i, nucleotide in enumerate(tata_seq):\n",
    "        if tata_pos + i < length:\n",
    "            seq[tata_pos + i] = nucleotide\n",
    "    \n",
    "    # Insert GC-rich region (CpG island-like) around position 80-120\n",
    "    gc_start = random.randint(75, 85)\n",
    "    gc_length = random.randint(30, 40)\n",
    "    for i in range(gc_length):\n",
    "        if gc_start + i < length:\n",
    "            seq[gc_start + i] = random.choice(['G', 'C', 'G', 'C', 'G', 'C', 'A', 'T'])  # 75% GC\n",
    "    \n",
    "    # Insert initiator element around position 130-140\n",
    "    inr_pos = random.randint(125, 135)\n",
    "    inr_patterns = ['TCAGTT', 'CCAATT', 'TCAATT', 'CCAGTT']\n",
    "    inr_seq = random.choice(inr_patterns)\n",
    "    for i, nucleotide in enumerate(inr_seq):\n",
    "        if inr_pos + i < length:\n",
    "            seq[inr_pos + i] = nucleotide\n",
    "    \n",
    "    return ''.join(seq)\n",
    "\n",
    "def generate_non_promoter_sequence(length: int = 256) -> str:\n",
    "    \"\"\"\n",
    "    Generate a non-promoter sequence (mostly random, avoiding promoter patterns).\n",
    "    \"\"\"\n",
    "    seq = generate_random_dna(length)\n",
    "    # Ensure no TATA box\n",
    "    seq = seq.replace('TATAAA', 'GGCCGG')\n",
    "    seq = seq.replace('TATATA', 'GGCCGG')\n",
    "    return seq\n",
    "\n",
    "# Generate example sequences\n",
    "print(\"Example Promoter Sequence:\")\n",
    "promoter_ex = generate_promoter_sequence(256)\n",
    "print(f\"  ...{promoter_ex[40:70]}... (TATA region)\")\n",
    "print(f\"  ...{promoter_ex[75:120]}... (GC-rich region)\")\n",
    "\n",
    "print(\"\\nExample Non-Promoter Sequence:\")\n",
    "non_promoter_ex = generate_non_promoter_sequence(256)\n",
    "print(f\"  ...{non_promoter_ex[40:70]}...\")\n",
    "print(f\"  ...{non_promoter_ex[75:120]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c695f52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PromoterDataset(Dataset):\n",
    "    \"\"\"Dataset for promoter prediction task.\"\"\"\n",
    "    \n",
    "    def __init__(self, num_samples: int = 1000, seq_length: int = 256, tokenizer: DNATokenizer = None):\n",
    "        self.num_samples = num_samples\n",
    "        self.seq_length = seq_length\n",
    "        self.tokenizer = tokenizer or DNATokenizer()\n",
    "        \n",
    "        # Generate balanced dataset\n",
    "        self.sequences = []\n",
    "        self.labels = []\n",
    "        \n",
    "        for i in range(num_samples):\n",
    "            if i < num_samples // 2:\n",
    "                # Promoter (positive)\n",
    "                seq = generate_promoter_sequence(seq_length)\n",
    "                label = 1\n",
    "            else:\n",
    "                # Non-promoter (negative)\n",
    "                seq = generate_non_promoter_sequence(seq_length)\n",
    "                label = 0\n",
    "            \n",
    "            self.sequences.append(seq)\n",
    "            self.labels.append(label)\n",
    "        \n",
    "        # Shuffle\n",
    "        combined = list(zip(self.sequences, self.labels))\n",
    "        random.shuffle(combined)\n",
    "        self.sequences, self.labels = zip(*combined)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.sequences[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Tokenize\n",
    "        tokens = self.tokenizer.encode(seq, max_length=self.seq_length)\n",
    "        \n",
    "        return {\n",
    "            'input_ids': tokens,\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Create datasets\n",
    "SEQ_LENGTH = 256  # Must be divisible by 2^local_layers (2^2 = 4)\n",
    "\n",
    "train_dataset = PromoterDataset(num_samples=2000, seq_length=SEQ_LENGTH, tokenizer=tokenizer)\n",
    "val_dataset = PromoterDataset(num_samples=400, seq_length=SEQ_LENGTH, tokenizer=tokenizer)\n",
    "test_dataset = PromoterDataset(num_samples=400, seq_length=SEQ_LENGTH, tokenizer=tokenizer)\n",
    "\n",
    "# Create dataloaders\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")\n",
    "\n",
    "# Check a batch\n",
    "batch = next(iter(train_loader))\n",
    "print(f\"\\nBatch shape: {batch['input_ids'].shape}\")\n",
    "print(f\"Labels shape: {batch['label'].shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d838bfd4",
   "metadata": {},
   "source": [
    "## 4. MergeDNA Model for Classification\n",
    "\n",
    "We'll adapt the MergeDNA architecture for sequence classification. The key idea is to use the encoder to get compressed representations, then add a classification head on top.\n",
    "\n",
    "The MergeDNA model uses **dynamic token merging** to compress long sequences efficiently, which is particularly useful for DNA sequences that can be thousands of base pairs long.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8b9874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import MergeDNA components\n",
    "from model.backbone import MergeDNAModel\n",
    "from model.encoder import LocalEncoder, LatentEncoder\n",
    "from mdna.utils import TransformerBlock, SinusoidalPositionalEmbedding\n",
    "\n",
    "class MergeDNAClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    MergeDNA model adapted for sequence classification.\n",
    "    Uses the encoder for feature extraction and adds a classification head.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int = 12,\n",
    "        dim: int = 64,\n",
    "        local_layers: int = 2,\n",
    "        latent_layers: int = 2,\n",
    "        heads: int = 4,\n",
    "        mlp_dim: int = 256,\n",
    "        num_classes: int = 2,\n",
    "        dropout: float = 0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dim = dim\n",
    "        self.local_layers = local_layers\n",
    "        \n",
    "        # Embedding layers\n",
    "        self.embedding = nn.Embedding(vocab_size, dim)\n",
    "        self.pos_emb = SinusoidalPositionalEmbedding(dim)\n",
    "        \n",
    "        # Encoder (from MergeDNA)\n",
    "        self.local_encoder = LocalEncoder(local_layers, dim, heads, mlp_dim, dropout)\n",
    "        self.latent_encoder = LatentEncoder(latent_layers, dim, heads, mlp_dim, dropout)\n",
    "        \n",
    "        # Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, mlp_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(mlp_dim, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: [Batch, Seq_Len] - token indices\n",
    "        Returns: [Batch, num_classes] - logits\n",
    "        \"\"\"\n",
    "        # Embed tokens\n",
    "        x_emb = self.embedding(x)  # [B, S, D]\n",
    "        x_emb = self.pos_emb(x_emb)\n",
    "        \n",
    "        # Encode - this reduces sequence length via token merging\n",
    "        x_local = self.local_encoder(x_emb)  # [B, S/4, D] with 2 local layers\n",
    "        x_latent = self.latent_encoder(x_local)  # [B, S/4, D]\n",
    "        \n",
    "        # Global pooling for classification\n",
    "        # Mean pooling over sequence dimension\n",
    "        x_pooled = x_latent.mean(dim=1)  # [B, D]\n",
    "        \n",
    "        # Classification\n",
    "        logits = self.classifier(x_pooled)  # [B, num_classes]\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "    def get_embeddings(self, x):\n",
    "        \"\"\"Get the latent embeddings (useful for visualization).\"\"\"\n",
    "        x_emb = self.embedding(x)\n",
    "        x_emb = self.pos_emb(x_emb)\n",
    "        x_local = self.local_encoder(x_emb)\n",
    "        x_latent = self.latent_encoder(x_local)\n",
    "        return x_latent\n",
    "\n",
    "# Initialize model\n",
    "model = MergeDNAClassifier(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    dim=64,\n",
    "    local_layers=2,\n",
    "    latent_layers=2,\n",
    "    heads=4,\n",
    "    mlp_dim=256,\n",
    "    num_classes=2,\n",
    "    dropout=0.1\n",
    ").to(device)\n",
    "\n",
    "# Count parameters\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "num_trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters: {num_params:,}\")\n",
    "print(f\"Trainable parameters: {num_trainable:,}\")\n",
    "\n",
    "# Test forward pass\n",
    "test_input = batch['input_ids'].to(device)\n",
    "test_output = model(test_input)\n",
    "print(f\"\\nInput shape: {test_input.shape}\")\n",
    "print(f\"Output shape: {test_output.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50bf2dbb",
   "metadata": {},
   "source": [
    "## 5. Training Loop\n",
    "\n",
    "Now let's train the model on our promoter prediction task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122dcc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, optimizer, criterion, device):\n",
    "    \"\"\"Train for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch in loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        logits = model(input_ids)\n",
    "        loss = criterion(logits, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        preds = logits.argmax(dim=-1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "    \n",
    "    return total_loss / len(loader), correct / total\n",
    "\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    \"\"\"Evaluate the model.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            logits = model(input_ids)\n",
    "            loss = criterion(logits, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            preds = logits.argmax(dim=-1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    return total_loss / len(loader), correct / total, all_preds, all_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c624d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "NUM_EPOCHS = 15\n",
    "LEARNING_RATE = 1e-3\n",
    "WEIGHT_DECAY = 1e-4\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS)\n",
    "\n",
    "# Training history\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "    'val_loss': [],\n",
    "    'val_acc': []\n",
    "}\n",
    "\n",
    "best_val_acc = 0.0\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "    val_loss, val_acc, _, _ = evaluate(model, val_loader, criterion, device)\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    \n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        # Save best model\n",
    "        torch.save(model.state_dict(), 'best_promoter_model.pt')\n",
    "    \n",
    "    print(f\"Epoch {epoch+1:2d}/{NUM_EPOCHS} | \"\n",
    "          f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | \"\n",
    "          f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(f\"Best validation accuracy: {best_val_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2c5375",
   "metadata": {},
   "source": [
    "## 6. Training Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557749c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Loss plot\n",
    "axes[0].plot(history['train_loss'], label='Train Loss', color='#2ecc71', linewidth=2)\n",
    "axes[0].plot(history['val_loss'], label='Val Loss', color='#e74c3c', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training & Validation Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy plot\n",
    "axes[1].plot(history['train_acc'], label='Train Acc', color='#2ecc71', linewidth=2)\n",
    "axes[1].plot(history['val_acc'], label='Val Acc', color='#e74c3c', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Training & Validation Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b7bee7",
   "metadata": {},
   "source": [
    "## 7. Test Set Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a58a647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model and evaluate on test set\n",
    "model.load_state_dict(torch.load('best_promoter_model.pt'))\n",
    "\n",
    "test_loss, test_acc, test_preds, test_labels = evaluate(model, test_loader, criterion, device)\n",
    "\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# Confusion matrix\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "cm = confusion_matrix(test_labels, test_preds)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,\n",
    "            xticklabels=['Non-Promoter', 'Promoter'],\n",
    "            yticklabels=['Non-Promoter', 'Promoter'])\n",
    "ax.set_xlabel('Predicted')\n",
    "ax.set_ylabel('Actual')\n",
    "ax.set_title('Confusion Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(test_labels, test_preds, \n",
    "                          target_names=['Non-Promoter', 'Promoter']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f953ad3",
   "metadata": {},
   "source": [
    "## 8. Inference Example\n",
    "\n",
    "Let's test the model on some new sequences to see how it performs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba16379",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_promoter(model, sequence: str, tokenizer: DNATokenizer, device, seq_length: int = 256):\n",
    "    \"\"\"\n",
    "    Predict whether a DNA sequence is a promoter.\n",
    "    \n",
    "    Returns:\n",
    "        dict with prediction label, probabilities, and confidence\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = tokenizer.encode(sequence, max_length=seq_length).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Predict\n",
    "    with torch.no_grad():\n",
    "        logits = model(tokens)\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        pred = logits.argmax(dim=-1).item()\n",
    "    \n",
    "    return {\n",
    "        'sequence_preview': sequence[:50] + '...' if len(sequence) > 50 else sequence,\n",
    "        'prediction': 'Promoter' if pred == 1 else 'Non-Promoter',\n",
    "        'confidence': probs[0, pred].item(),\n",
    "        'probabilities': {\n",
    "            'non_promoter': probs[0, 0].item(),\n",
    "            'promoter': probs[0, 1].item()\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Test on new sequences\n",
    "print(\"=\" * 60)\n",
    "print(\"INFERENCE EXAMPLES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Generate a new promoter sequence\n",
    "new_promoter = generate_promoter_sequence(256)\n",
    "result = predict_promoter(model, new_promoter, tokenizer, device)\n",
    "print(f\"\\n[Generated Promoter Sequence]\")\n",
    "print(f\"  Preview: {result['sequence_preview']}\")\n",
    "print(f\"  Prediction: {result['prediction']}\")\n",
    "print(f\"  Confidence: {result['confidence']:.4f}\")\n",
    "\n",
    "# Generate a new non-promoter sequence\n",
    "new_non_promoter = generate_non_promoter_sequence(256)\n",
    "result = predict_promoter(model, new_non_promoter, tokenizer, device)\n",
    "print(f\"\\n[Generated Non-Promoter Sequence]\")\n",
    "print(f\"  Preview: {result['sequence_preview']}\")\n",
    "print(f\"  Prediction: {result['prediction']}\")\n",
    "print(f\"  Confidence: {result['confidence']:.4f}\")\n",
    "\n",
    "# Test with a sequence containing clear TATA box\n",
    "tata_sequence = \"GCGCGCGCGCATATATATATATAAAAAAAATCAGTTGCGCGCGCGCGCGC\" + generate_random_dna(206)\n",
    "result = predict_promoter(model, tata_sequence, tokenizer, device)\n",
    "print(f\"\\n[Sequence with TATA-like motif]\")\n",
    "print(f\"  Preview: {result['sequence_preview']}\")\n",
    "print(f\"  Prediction: {result['prediction']}\")\n",
    "print(f\"  Confidence: {result['confidence']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa83387",
   "metadata": {},
   "source": [
    "## 9. Embedding Visualization\n",
    "\n",
    "Let's visualize how the model separates promoter and non-promoter sequences in the latent space using t-SNE.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f361791",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Get embeddings for test set\n",
    "model.eval()\n",
    "all_embeddings = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        labels = batch['label']\n",
    "        \n",
    "        # Get latent embeddings and pool\n",
    "        embeddings = model.get_embeddings(input_ids)  # [B, S/4, D]\n",
    "        pooled = embeddings.mean(dim=1)  # [B, D]\n",
    "        \n",
    "        all_embeddings.append(pooled.cpu().numpy())\n",
    "        all_labels.extend(labels.numpy())\n",
    "\n",
    "all_embeddings = np.vstack(all_embeddings)\n",
    "all_labels = np.array(all_labels)\n",
    "\n",
    "# Apply t-SNE\n",
    "print(\"Computing t-SNE projection...\")\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "embeddings_2d = tsne.fit_transform(all_embeddings)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "colors = ['#3498db', '#e74c3c']\n",
    "labels_text = ['Non-Promoter', 'Promoter']\n",
    "\n",
    "for i in range(2):\n",
    "    mask = all_labels == i\n",
    "    ax.scatter(embeddings_2d[mask, 0], embeddings_2d[mask, 1], \n",
    "               c=colors[i], label=labels_text[i], alpha=0.6, s=30)\n",
    "\n",
    "ax.set_xlabel('t-SNE Component 1')\n",
    "ax.set_ylabel('t-SNE Component 2')\n",
    "ax.set_title('t-SNE Visualization of Learned Embeddings')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nEmbedding shape: {all_embeddings.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6eac42",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we demonstrated:\n",
    "\n",
    "1. **DNA Tokenization**: Converting nucleotide sequences (A, T, C, G) into token indices\n",
    "2. **Synthetic Data Generation**: Creating promoter-like sequences with characteristic motifs (TATA box, GC-rich regions, initiator elements)\n",
    "3. **MergeDNA for Classification**: Adapting the MergeDNA encoder architecture for binary classification by adding a pooling layer and classification head\n",
    "4. **Training Pipeline**: Standard PyTorch training loop with validation monitoring\n",
    "5. **Evaluation**: Confusion matrix and classification metrics on held-out test set\n",
    "6. **Embedding Visualization**: t-SNE projection showing learned representations\n",
    "\n",
    "### Key Features of MergeDNA for Genomics:\n",
    "\n",
    "- **Dynamic Token Merging**: Efficiently compresses long DNA sequences while preserving important features\n",
    "- **Hierarchical Encoding**: Local encoder captures short-range patterns (motifs), latent encoder captures long-range dependencies\n",
    "- **Scalability**: The merging mechanism allows processing of very long sequences (thousands of base pairs)\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- Use real promoter datasets (e.g., EPDnew, ENCODE)\n",
    "- Experiment with different sequence lengths and model sizes\n",
    "- Add attention visualization to identify important motifs\n",
    "- Fine-tune on organism-specific promoter data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0ab294",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
